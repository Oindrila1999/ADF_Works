{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "ADFDemoOindrila"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/cache_sink_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "emp_table_ds",
								"type": "DatasetReference"
							},
							"name": "emptable"
						},
						{
							"dataset": {
								"referenceName": "emp_details_1_ds",
								"type": "DatasetReference"
							},
							"name": "empdetails1"
						},
						{
							"dataset": {
								"referenceName": "country_table_ds",
								"type": "DatasetReference"
							},
							"name": "countrytable"
						}
					],
					"sinks": [
						{
							"name": "cacheMaxEmpID"
						},
						{
							"name": "cacheCountryName"
						},
						{
							"dataset": {
								"referenceName": "emp_table_ds",
								"type": "DatasetReference"
							},
							"name": "emptableinserts"
						}
					],
					"transformations": [
						{
							"name": "generateempid"
						},
						{
							"name": "derivedColumnempid"
						}
					],
					"scriptLines": [
						"source(output(",
						"          maxEmpID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     query: 'select max(empid) as maxEmpID from emp_table',",
						"     format: 'query') ~> emptable",
						"source(output(",
						"          name as string,",
						"          gender as string,",
						"          country as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empdetails1",
						"source(output(",
						"          code as string,",
						"          country as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> countrytable",
						"empdetails1 keyGenerate(output(empid as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> generateempid",
						"generateempid derive(empid = cacheMaxEmpID#output().maxEmpID + empid,",
						"          country = cacheCountryName#lookup(country).country) ~> derivedColumnempid",
						"emptable sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: false,",
						"     saveOrder: 0,",
						"     mapColumn(",
						"          maxEmpID",
						"     )) ~> cacheMaxEmpID",
						"countrytable sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     keys:['code'],",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: false,",
						"     saveOrder: 0,",
						"     mapColumn(",
						"          code,",
						"          country",
						"     )) ~> cacheCountryName",
						"derivedColumnempid sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          name as string,",
						"          gender as string,",
						"          country as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          name,",
						"          gender,",
						"          country",
						"     )) ~> emptableinserts"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/conditional_split_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "emopdetails_agg_dataflow",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "split_dataflow_ds",
								"type": "DatasetReference"
							},
							"name": "ITemp"
						},
						{
							"dataset": {
								"referenceName": "split_dataflow_ds",
								"type": "DatasetReference"
							},
							"name": "PayrollEmp"
						},
						{
							"dataset": {
								"referenceName": "split_dataflow_ds",
								"type": "DatasetReference"
							},
							"name": "HREmp"
						},
						{
							"dataset": {
								"referenceName": "split_dataflow_ds",
								"type": "DatasetReference"
							},
							"name": "DefaultEmp"
						}
					],
					"transformations": [
						{
							"name": "splitBasedOnDept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          name as string,",
						"          country as string,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"employee split(equals(deptname, 'IT'),",
						"     equals(deptname,'HR'),",
						"     equals(deptname,'PAYROLL'),",
						"     disjoint: false) ~> splitBasedOnDept@(IT, HR, PAYROLL, Otherdept)",
						"splitBasedOnDept@IT sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['ITEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> ITemp",
						"splitBasedOnDept@PAYROLL sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['PayrollEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> PayrollEmp",
						"splitBasedOnDept@HR sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['HREmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> HREmp",
						"splitBasedOnDept@Otherdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['OtherEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> DefaultEmp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_sc_1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Split the records as per data validation.",
				"folder": {
					"name": "Scenario-based-question/scenario-1"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_source_sc_1",
								"type": "DatasetReference"
							},
							"name": "salessource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_sink_sc_1",
								"type": "DatasetReference"
							},
							"name": "tblsalessink"
						},
						{
							"dataset": {
								"referenceName": "ds_sink_bad_sc_1",
								"type": "DatasetReference"
							},
							"name": "tblsalesbadsink"
						}
					],
					"transformations": [
						{
							"name": "splitbasedondatavalidation"
						},
						{
							"name": "derivedfilename"
						},
						{
							"name": "derivedfileName1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          saleDate as string,",
						"          saleItem as string,",
						"          country as string,",
						"          quantity as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> salessource",
						"salessource split(!isNull(toDate(saleDate,'dd-MMM-yyyy')),",
						"     disjoint: false) ~> splitbasedondatavalidation@(goodRec, badRec)",
						"splitbasedondatavalidation@goodRec derive(FileName = 'salesIND-2020-MAY-01.csv',",
						"          saleDate = toDate(saleDate, 'dd-MMM-yyyy')) ~> derivedfilename",
						"splitbasedondatavalidation@badRec derive(BadFile = 'sales-bad-record.csv') ~> derivedfileName1",
						"derivedfilename sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          saleDate as date,",
						"          saleItem as string,",
						"          country as string,",
						"          quantity as integer,",
						"          filename as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          saleDate,",
						"          saleItem,",
						"          country,",
						"          quantity,",
						"          filename = FileName",
						"     )) ~> tblsalessink",
						"derivedfileName1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          saleDate,",
						"          saleItem,",
						"          country,",
						"          quantity,",
						"          filename = BadFile",
						"     )) ~> tblsalesbadsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_sc_5')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Scenario-based-question/scenario-5"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_source_sc_5",
								"type": "DatasetReference"
							},
							"name": "fixedsource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_sink_sc_5",
								"type": "DatasetReference"
							},
							"name": "sink"
						}
					],
					"transformations": [
						{
							"name": "splitColumns"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Column_1 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> fixedsource",
						"fixedsource derive(Id = substring(Column_1,1,4),",
						"          Name = substring(Column_1,5,10),",
						"          State = substring(Column_1,15,2),",
						"          Contact = substring(Column_1,18)) ~> splitColumns",
						"splitColumns sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Fixed_split.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Id,",
						"          Name,",
						"          State,",
						"          Contact",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/derived_column_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empdataset_derived_df",
								"type": "DatasetReference"
							},
							"name": "empdetails"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "emp_details_table_ds",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumnCountry"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptname as string,",
						"          age as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empdetails",
						"empdetails derive(country = iifNull(country,'No country present'),",
						"          Age_group = iif(age < 25 , 'young', iif((age >= 25 && age < 50), 'adult', 'senior'))) ~> derivedColumnCountry",
						"derivedColumnCountry sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as integer,",
						"          ename as string,",
						"          country as string,",
						"          deptname as string,",
						"          age as integer,",
						"          AGE_GROUP as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     truncate:true,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id = empid,",
						"          ename = name,",
						"          country,",
						"          deptname,",
						"          age,",
						"          AGE_GROUP = Age_group",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/exists_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empdataset_adls",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "departmentdataset_adls",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "exists_dataset_sink",
								"type": "DatasetReference"
							},
							"name": "existssink"
						},
						{
							"dataset": {
								"referenceName": "exists_dataset_sink",
								"type": "DatasetReference"
							},
							"name": "notexixtssink"
						}
					],
					"transformations": [
						{
							"name": "deptempexists"
						},
						{
							"name": "deptnoempexists"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptid as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          deptid as integer,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"department, employee exists(department@deptid == employee@deptid,",
						"     negate:false,",
						"     broadcast: 'auto')~> deptempexists",
						"department, employee exists(department@deptid == employee@deptid,",
						"     negate:true,",
						"     broadcast: 'auto')~> deptnoempexists",
						"deptempexists sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          name as string,",
						"          sales as string,",
						"          date as string",
						"     ),",
						"     partitionFileNames:['dept_where_emp_exixts.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> existssink",
						"deptnoempexists sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          name as string,",
						"          sales as string,",
						"          date as string",
						"     ),",
						"     partitionFileNames:['dept_where_no_emp_exixts.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> notexixtssink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/filter_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "emp_dept_dataset",
								"type": "DatasetReference"
							},
							"name": "empdept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "sinkdataset_dataflow",
								"type": "DatasetReference"
							},
							"name": "filteredemp"
						}
					],
					"transformations": [
						{
							"name": "filterdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          country as string,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> empdept",
						"empdept filter(equals(deptname,'Payroll')) ~> filterdept",
						"filterdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          name as string,",
						"          sales as string,",
						"          date as string",
						"     ),",
						"     partitionFileNames:['Payrollemp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> filteredemp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/flatten_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "flatten_trans_dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "flatten_dataflow_out_ds",
								"type": "DatasetReference"
							},
							"name": "flattenoutsink"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          name as string,",
						"          skills as string[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 foldDown(unroll(skills),",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          skills",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Flatten_array_data.json'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          skills",
						"     ),",
						"     partitionBy('hash', 1)) ~> flattenoutsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/group_by_powerquery')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "empdataset_derived_df",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> empdataset_derived_df",
							"dataset": {
								"referenceName": "empdataset_derived_df",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared empdataset_derived_df = let AdfDoc = AzureStorage.DataLakeContents(\"https://storageadlsdemooin.dfs.core.windows.net/adfdemooinadls/input/emp_for_derived.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]),\r\n  #\"Changed column type\" = Table.TransformColumnTypes(PromotedHeaders, {{\"empid\", Int64.Type}, {\"age\", Int64.Type}}) in  #\"Changed column type\";\r\nshared UserQuery = let Source = #\"empdataset_derived_df\",\r\n  #\"Grouped rows\" = Table.Group(Source, {\"deptname\"}, {{\"TotalEmp\", each Table.RowCount(_), Int64.Type}}) in #\"Grouped rows\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/join_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empdataset_adls",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "departmentdataset_adls",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "emp_dept_dataset",
								"type": "DatasetReference"
							},
							"name": "empdept"
						}
					],
					"transformations": [
						{
							"name": "joinempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          name as string,",
						"          country as string,",
						"          deptid as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          deptid as string,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee, department join(employee@deptid == department@deptid,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinempdept",
						"joinempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          empname as string,",
						"          country as string,",
						"          deptname as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          empname = name,",
						"          country,",
						"          deptname",
						"     )) ~> empdept"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/lookup_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "deptDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "out_lkp_trans_dataset",
								"type": "DatasetReference"
							},
							"name": "empwithdeptname"
						}
					],
					"transformations": [
						{
							"name": "lookupdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          name as string,",
						"          country as string,",
						"          deptid as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          deptid as string,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee, department lookup(employee@deptid == department@deptid,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookupdept",
						"lookupdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          name as string,",
						"          sales as string,",
						"          date as string",
						"     ),",
						"     partitionFileNames:['Emp_with_dept_name.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> empwithdeptname"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/merge_queries_powerquery')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "WranglingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"name": "empdataset_adls",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> empdataset_adls",
							"dataset": {
								"referenceName": "empdataset_adls",
								"type": "DatasetReference"
							}
						},
						{
							"name": "departmentdataset_adls",
							"script": "source(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false) ~> departmentdataset_adls",
							"dataset": {
								"referenceName": "departmentdataset_adls",
								"type": "DatasetReference"
							}
						}
					],
					"script": "section Section1;\r\nshared empdataset_adls = let AdfDoc = AzureStorage.DataLakeContents(\"https://storageadlsdemooin.dfs.core.windows.net/adfdemooinadls/input/employees.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared departmentdataset_adls = let AdfDoc = AzureStorage.DataLakeContents(\"https://storageadlsdemooin.dfs.core.windows.net/adfdemooinadls/input/department.csv\"),Csv = Csv.Document(AdfDoc, [Delimiter = \",\", Encoding = TextEncoding.Utf8, QuoteStyle = QuoteStyle.Csv]), PromotedHeaders = Table.PromoteHeaders(Csv, [PromoteAllScalars = true]) in  PromotedHeaders;\r\nshared UserQuery = let Source = #\"empdataset_adls\",\r\n  #\"Merged queries\" = Table.NestedJoin(Source, {\"deptid\"}, departmentdataset_adls, {\"deptid\"}, \"departmentdataset_adls\", JoinKind.Inner),\r\n  #\"Expanded departmentdataset_adls\" = Table.ExpandTableColumn(#\"Merged queries\", \"departmentdataset_adls\", {\"deptname\"}, {\"deptname\"}) in #\"Expanded departmentdataset_adls\";\r\n",
					"documentLocale": "en-us"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/new_branch_work_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "deptDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "empcountdataset_sink",
								"type": "DatasetReference"
							},
							"name": "empcountsink"
						},
						{
							"dataset": {
								"referenceName": "joineddataset_sink",
								"type": "DatasetReference"
							},
							"name": "joineddatasink"
						}
					],
					"transformations": [
						{
							"name": "countemp"
						},
						{
							"name": "joinempdept"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptid as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          deptid as integer,",
						"          deptname as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employee aggregate(groupBy(deptid),",
						"     count_of_emp = count(empid)) ~> countemp",
						"employee, department join(employee@deptid == department@deptid,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> joinempdept",
						"countemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['Emp_count_dept_wise.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> empcountsink",
						"joinempdept sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptname as string,",
						"          Employee_count as string",
						"     ),",
						"     partitionFileNames:['Joined_emp_dept.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> joineddatasink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/parse_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_tbl_employee",
								"type": "DatasetReference"
							},
							"name": "emptable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "out_parse_trans",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parseskills"
						},
						{
							"name": "parseaddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          empname as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> emptable",
						"emptable parse(Parseskills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseskills",
						"parseskills parse(Parseaddress = address ? (city as string,",
						"          country as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parseaddress",
						"parseaddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Parse_trans_out.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          empname,",
						"          skill1 = Parseskills.skill1,",
						"          skill2 = Parseskills.skill2,",
						"          skill3 = Parseskills.skill3,",
						"          city = Parseaddress.city,",
						"          country = Parseaddress.country",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pivot_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "emp_source_dataset_for_pivot",
								"type": "DatasetReference"
							},
							"name": "empsource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "pivot_out_sink",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivotgender"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          name as string,",
						"          gender as string,",
						"          country as string,",
						"          department as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> empsource",
						"empsource pivot(groupBy(department),",
						"     pivotBy(gender),",
						"     {} = count(id),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivotgender",
						"pivotgender sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          deptid as string,",
						"          count_of_emp as string",
						"     ),",
						"     partitionFileNames:['pivotted_output.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/rank_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "input_ds_window_trans",
								"type": "DatasetReference"
							},
							"name": "empdata"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "out_ds_for_rank",
								"type": "DatasetReference"
							},
							"name": "rankedsink"
						}
					],
					"transformations": [
						{
							"name": "rankOnsalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as integer,",
						"          name as string,",
						"          gender as string,",
						"          salary as integer,",
						"          dept as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> empdata",
						"empdata rank(desc(salary, true),",
						"     output(salary_ranking as long)) ~> rankOnsalary",
						"rankOnsalary sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['Out_ranked_sal.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          gender,",
						"          salary,",
						"          dept,",
						"          salary_ranking",
						"     ),",
						"     partitionBy('hash', 1)) ~> rankedsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/select_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "emp_selected_dataset",
								"type": "DatasetReference"
							},
							"name": "selectedcolssink"
						}
					],
					"transformations": [
						{
							"name": "selectcolumns"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptid as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"employee select(mapColumn(",
						"          employee_id = empid,",
						"          country,",
						"          emp_name = name",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> selectcolumns",
						"selectcolumns sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'parquet',",
						"     partitionFileNames:['selected_emp_data.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          employee_id,",
						"          country,",
						"          emp_name",
						"     ),",
						"     partitionBy('hash', 1)) ~> selectedcolssink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/sort_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataset_lkp_transformation",
								"type": "DatasetReference"
							},
							"name": "employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "emp_sort_trans_dataset",
								"type": "DatasetReference"
							},
							"name": "emporderedsink"
						}
					],
					"transformations": [
						{
							"name": "sortdeptidcntry"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptid as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"employee sort(asc(deptid, true),",
						"     desc(country, true)) ~> sortdeptidcntry",
						"sortdeptidcntry sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          empid as integer,",
						"          name as string,",
						"          country as string,",
						"          deptid as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          name,",
						"          country,",
						"          deptid",
						"     )) ~> emporderedsink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/stringify_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_json_stringify",
								"type": "DatasetReference"
							},
							"name": "jsondata"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "stringify_json_out",
								"type": "DatasetReference"
							},
							"name": "stringifysink"
						}
					],
					"transformations": [
						{
							"name": "stringify"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          skills as string[],",
						"          contact as (mobile as string, landline as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> jsondata",
						"jsondata stringify(contactStringify = contact ? string,",
						"     format: 'json') ~> stringify",
						"stringify derive(contactStringify = toString(contactStringify)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['stringify_out.json'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> stringifysink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/surr_key_trans_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "input_dataset_sugg_key",
								"type": "DatasetReference"
							},
							"name": "source"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "surr_ds_output",
								"type": "DatasetReference"
							},
							"name": "surrkeysink"
						}
					],
					"transformations": [
						{
							"name": "surrogateKeygenerate"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Country as string,",
						"          Emp as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source",
						"source keyGenerate(output(empid as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKeygenerate",
						"surrogateKeygenerate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          id as string,",
						"          name as string,",
						"          sales as string,",
						"          date as string",
						"     ),",
						"     partitionFileNames:['surr_key_out.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> surrkeysink"
					]
				}
			},
			"dependsOn": []
		}
	]
}